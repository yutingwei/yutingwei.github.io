<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">Bio</a></div>
<div class="menu-item"><a href="publications.html">Research</a></div>
<div class="menu-item"><a href="publications-category.html" class="current">Research&nbsp;by&nbsp;topic</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="experience.html">Talks</a></div>
<div class="menu-item"><a href="group.html">Group</a></div>
</td>
<td id="layout-content">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7Y30K6NVVF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-7Y30K6NVVF');
</script>
<h1>Research by Topic</h1>
<h2>High Dimensional Statistics and Learning </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2506.24042">Faster Diffusion Models via Higher-Order Approximation.</a> </p>
<ul>
<li><p>Gen Li∗, Yuchen Zhou∗, Yuting Wei, Yuxin Chen (∗=equal contribution)</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2506.05200">Transformers meet in-context learning: A universal approximation theory.</a></p>
<ul>
<li><p>Gen Li∗, Yuchen Jiao∗, Yu Huang, Yuting Wei, Yuxin Chen </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2504.05300">Dimension-free convergence of diffusion models for approximate Gaussian mixtures.</a></p>
<ul>
<li><p>Gen Li∗, Changxiao Cai∗, Yuting Wei </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.18784">Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality.</a></p>
<ul>
<li><p>Zhihan Huang, Yuting Wei, Yuxin Chen</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2401.03923">A non-asymptotic distributional theory of approximate message passing for sparse and robust regression.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuting Wei</p>
</li>
<li><p><b>Paper awarded for 2024 ICSA Junior Researcher Award</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.04760">Stochastic Runge-Kutta methods: Provable acceleration of diffusion models.</a></p>
<ul>
<li><p>Yuchen Wu, Yuxin Chen, Yuting Wei</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/DiffusionODE.pdf">A sharp convergence theory for the probability flow ODEs of diffusion models.</a></p>
<ul>
<li><p>Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2402.07802">Towards a mathematical theory for consistency training in diffusion models.</a> <br /></p>
<ul>
<li><p>Gen Li∗, Zhihan Huang∗, Yuting Wei  </p>
</li>
<li><p>AISTATS, 2025</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2403.01639">Theoretical insights for diffusion guidance: A case study for Gaussian mixture models.</a> <br /></p>
<ul>
<li><p>Yuchen Wu, Minshuo Chen, Zihao Li, Mengdi Wang, Yuting Wei </p>
</li>
<li><p>Short version accepted to ICML, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2403.03852">Accelerating convergence of score-based diffusion models, provably.</a> <br /></p>
<ul>
<li><p>Gen Li∗, Yu Huang∗, Timofey Efimov, Yuting Wei, Yuejie Chi, Yuxin Chen  </p>
</li>
<li><p>ICML, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2306.09251">Towards faster non-asymptotic convergence for diffusion-based generative models.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/Yuting_AMP_PNAS_full.pdf">Approximate message passing from random initialization with applications to Z2 synchronization.</a> <br /></p>
<ul>
<li><p>Gen Li, Wei Fan, Yuting Wei</p>
</li>
<li><p>Proceedings of the National Academy of Sciences (PNAS), 2023</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2208.03313">A non-asymptotic framework for approximate message passing in spiked models.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuting Wei</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2205.12937">Mitigating multiple descents: A model-agnostic framework for risk monotonization.</a> <br /></p>
<ul>
<li><p>Pratik Patil, Arun Kumar Kuchibhotla, Yuting Wei, Alessandro Rinaldo</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.09502">Minimum L1 interpolators: Precise asymptotics and multiple descent.</a> <br /></p>
<ul>
<li><p>Yue Li, Yuting Wei</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/lasso-general.pdf">The Lasso with general Gaussian designs with applications to hypothesis testing.</a> <br /></p>
<ul>
<li><p>Michael Celentano, Andrea Montanari, Yuting Wei (alphabetical order) <a href="documents/slides/lasso.pdf">(slides)</a></p>
</li>
<li><p>Annals of Statistics, 2023</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/uniform.pdf">Uniform consistency of cross validation estimators for high-dimensional ridge regression.</a> <br /></p>
<ul>
<li><p>Pratik Patil, Yuting Wei, Alessandro Rinaldo, Ryan Tibshirani</p>
</li>
<li><p>AISTATS, oral presentation, 2021</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.16384">Sharp statistical guarantees for adversarially robust Gaussian classification.</a> <br /></p>
<ul>
<li><p>Chen Dan, Yuting Wei, Pradeep Ravikumar</p>
</li>
<li><p>ICML, 2020</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/early-stopping.pdf">Early stopping for kernel boosting algorithms: A general analysis with localized complexities.</a> <br /></p>
<ul>
<li><p>Yuting Wei∗, Fanny Yang∗ and Martin Wainwright  </p>
</li>
<li><p>Short version accepted to NeurIPS, spotlight presentation, 2017</p>
</li>
<li><p>IEEE Transactions on Information Theory, 2019</p>
</li>
</ul>

</li>
</ul>
<h2>Reinforcement Learning and Strategic Games</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2507.14444">Statistical and Algorithmic Foundations of Reinforcement Learning.</a> </p>
<ul>
<li><p>Yuejie Chi, Yuxin Chen, Yuting Wei</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2505.03710">Actor-critics can achieve optimal sample efficiency.</a></p>
<ul>
<li><p>Kevin Tan∗, Wei Fan∗, Yuting Wei </p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2502.13822">Uncertainty quantification for Markov chains with application to temporal difference learning.</a></p>
<ul>
<li><p>Weichen Wu, Yuting Wei, Alessandro Rinaldo</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2410.16106">Statistical inference for temporal difference learning with linear function approximation.</a></p>
<ul>
<li><p>Weichen Wu, Gen Li, Yuting Wei, Alessandro Rinaldo</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/Hybrid_RL.pdf">Hybrid reinforcement learning breaks sample size barriers in linear MDPs.</a></p>
<ul>
<li><p>Kevin Tan, Wei Fan, and Yuting Wei</p>
</li>
<li><p>NeurIPS, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2311.00201">Federated Natural Policy Gradient and Actor Critic Methods for Multi-task Reinforcement Learning.</a></p>
<ul>
<li><p>Tong Yang, Shicong Cen, Yuting Wei, Yuxin Chen, Yuejie Chi</p>
</li>
<li><p>NeurIPS, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2204.05275">Settling the sample complexity of model-based offline reinforcement learning.</a> <br /></p>
<ul>
<li><p>Gen Li, Laixi Shi, Yuxin Chen, Yuejie Chi, Yuting Wei</p>
</li>
<li><p>Annals of Statistics, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.19001">High-probability sample complexities for policy evaluation with linear function approximation.</a></p>
<ul>
<li><p>Gen Li∗, Weichen Wu∗, Yuejie Chi, Cong Ma, Alessandro Rinaldo, Yuting Wei</p>
</li>
<li><p>IEEE Transactions on Information Theory, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2305.16589">The curious price of distributional robustness in reinforcement learning with a generative model.</a><br /></p>
<ul>
<li><p>Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, Yuejie Chi</p>
</li>
<li><p>Short version accepted to NeurIPS, 2023</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2102.06548">Is Q-Learning minimax optimal? A tight sample complexity analysis.</a> <br /></p>
<ul>
<li><p>Gen Li, Changxiao Cai, Yuxin Chen, Yuting Wei, Yuejie Chi</p>
</li>
<li><p>Short version accepted to ICML, 2021</p>
</li>
<li><p>Operations Research, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2005.12900">Breaking the sample size barrier in model-based reinforcement learning with a generative model.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen</p>
</li>
<li><p>Short version accepted to NeurIPS, 2020 <a href="documents/slides/model-based-rl-slides.pdf">(slides)</a></p>
</li>
<li><p>Operations Research, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/Markov_games_simulator.pdf">Minimax-optimal multi-agent RL in zero-sum Markov games with a generative model.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuejie Chi, Yuting Wei, Yuxin Chen</p>
</li>
<li><p>NeurIPS, oral presentation, 2023</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/EntropyGame.pdf">Fast policy extragradient methods for competitive games with entropy regularization.</a> <br /></p>
<ul>
<li><p>Shicong Cen, Yuting Wei, Yuejie Chi</p>
</li>
<li><p>Short version accepted to NeurIPS, 2021</p>
</li>
<li><p>Journal of Machine Learning Research, 2024</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2202.13890">Pessimistic Q-Learning for offline reinforcement learning: Towards optimal sample complexity.</a> <br /></p>
<ul>
<li><p>Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi</p>
</li>
<li><p>ICML, 2022</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/SoftmaxPG_LB.pdf">Softmax policy gradient methods can take exponential time to converge.</a><br /></p>
<ul>
<li><p>Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen</p>
</li>
<li><p>Short version accepted to Conference on Learning Theory (COLT), 2021</p>
</li>
<li><p>Mathematical Programming, 2023</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2105.08024">Sample-efficient reinforcement learning is feasible for linearly realizable MDPs with limited revisiting.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuxin Chen, Yuejie Chi, Yuantao Gu, Yuting Wei</p>
</li>
<li><p>NeurIPS, 2021 <a href="documents/slides/Linear_realizability_slides.pdf">(slides)</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2007.06558">Fast global convergence of natural policy gradient methods with entropy regularization.</a> <br /></p>
<ul>
<li><p>Shicong Cen, Chen Cheng, Yuxin Chen, Yuting Wei, Yuejie Chi</p>
</li>
<li><p>Operations Research, 2022 <b>(INFORMS George Nicholson award finalist)</b></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.03041">Sample complexity of asynchronous Q-learning: sharper analysis and variance reduction.</a> <br /></p>
<ul>
<li><p>Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen</p>
</li>
<li><p>Short version accepted to NeurIPS, 2020 <a href="http://www.princeton.edu/~yc5/slides/Async_Qlearning_slides.pdf">(slides)</a></p>
</li>
<li><p>IEEE Transactions on Information Theory, 2021</p>
</li>
</ul>

</li>
</ul>
<h2>Valid and Efficient Statistical Inference </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2012.02717">Derandomizing Knockoffs.</a><br /></p>
<ul>
<li><p>Zhimei Ren, Yuting Wei, Emmanuel Candès</p>
</li>
<li><p>Journal of the American Statistical Association, 2023 <a href="https://zhimeir.github.io/adaptiveKnockoffs/">(code)</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2001.04620">Tackling small eigen-gaps: Fine-grained eigenvector estimation and inference under heteroscedastic noise.</a> <br /></p>
<ul>
<li><p>Chen Cheng, Yuting Wei, Yuxin Chen</p>
</li>
<li><p>IEEE Transactions on Information Theory, 2021</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2010.01289">Randomized tests for high-dimensional regression: A more efficient and powerful solution.</a> <br /></p>
<ul>
<li><p>Yue Li, Ilmun Kim, Yuting Wei</p>
</li>
<li><p>NeurIPS, 2020</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/cone2018.pdf">The geometry of hypothesis testing over convex cones: Generalized likelihood tests and minimax radii.</a> <br /></p>
<ul>
<li><p>Yuting Wei, Martin Wainwright and Adityanand Guntuboyina</p>
</li>
<li><p>The Annals of Statistics, 2019</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1803.07763">From Gauss to Kolmogorov: Localized measures of complexity for ellipses.</a> <br /></p>
<ul>
<li><p>Yuting Wei, Billy Fang and Martin Wainwright</p>
</li>
<li><p>Electronic Journal of Statistics, 2020</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1712.00711">The local geometry of testing in ellipses: Tight control via localized Kolmogorov widths.</a><br /></p>
<ul>
<li><p>Yuting Wei and Martin Wainwright</p>
</li>
<li><p>IEEE Transactions on Information Theory, 2020</p>
</li>
</ul>

</li>
</ul>
<h2>Shape Constrained Inference </h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2012.00714">Debiasing evaluations that are biased by evaluations.</a> <br /></p>
<ul>
<li><p>Jingyan Wang, Ivan Stelmakh, Yuting Wei, Nihar B. Shah</p>
</li>
<li><p>Short version accepted to AAAI Conference on Artificial Intelligence, 2021</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="documents/paper/cone2018.pdf">The geometry of hypothesis testing over convex cones: Generalized likelihood tests and minimax radii.</a> <br /></p>
<ul>
<li><p>Yuting Wei, Martin Wainwright and Adityanand Guntuboyina</p>
</li>
<li><p>The Annals of Statistics, 2019</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1508.03744">Adaptive estimation of planar convex sets.</a> <br /></p>
<ul>
<li><p>Tony Cai, Adityanand Guntuboyina and Yuting Wei (alphabetical order). </p>
</li>
<li><p>The Annals of Statistics, 2018</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="http://ieeexplore.ieee.org/document/7541786/">Sharp minimax bounds for testing discrete
monotone distributions.</a> <br /></p>
<ul>
<li><p>Yuting Wei and Martin Wainwright</p>
</li>
<li><p>International Symposium on Information Theory (ISIT), 2016</p>
</li>
</ul>

</li>
</ul>
<h2>Applications in Bioinformatics and Genomics </h2>
<ul>
<li><p><a href="https://www.biorxiv.org/content/10.1101/2025.09.12.675662v1.abstract">Predicting the unseen: a diffusion-based debiasing framework for transcriptional response prediction at single-cell resolution.</a></p>
<ul>
<li><p>Ergan Shang, Yuting Wei, Kathryn Roeder</p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://www.pnas.org/content/118/10/e2024383118">Integration and transfer learning of single-cell transcriptomes via cFIT.</a> <br /></p>
<ul>
<li><p>Minshi Peng, Yue Li, Brie Wamsley, Yuting Wei, Kathryn Roeder</p>
</li>
<li><p>Proceedings of the National Academy of Sciences (PNAS), 2021 <a href="https://github.com/pengminshi/cFIT,">(code)</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="https://www.biorxiv.org/content/10.1101/2021.02.06.430067v1">Cell type hierarchy reconstruction via reconciliation of multi-resolution cluster tree.</a> <br /></p>
<ul>
<li><p>Minshi Peng, Brie Wamsley, Andrew Elkins, Daniel M Geschwind, Yuting Wei, Kathryn Roeder</p>
</li>
<li><p>Nucleic Acids Research, 2021 <a href="https://github.com/pengminshi/MRtree">(code)</a></p>
</li>
</ul>

</li>
</ul>
<ul>
<li><p><a href="http://www.sciencedirect.com/science/article/pii/S0022175914000532">MHC binding prediction with KernelRLSpan and its variations.</a></p>
<ul>
<li><p>Wen-Jun Shen∗, Yuting Wei∗, Xin Guo∗, Stephen Smale, Hau-San Wong and Shuai Cheng Li  </p>
</li>
<li><p>Journal of Immunological Methods, 2014</p>
</li>
</ul>

</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2025-10-15 23:37:44 EDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
